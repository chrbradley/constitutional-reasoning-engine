{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitutional Reasoning Experiment - Statistical Deep-Dive\n",
    "\n",
    "**Purpose:** Rigorous statistical analysis and hypothesis testing\n",
    "\n",
    "This notebook tests specific hypotheses about constitutional reasoning integrity:\n",
    "1. Are differences between models statistically significant?\n",
    "2. Is the bad-faith vs. honest gap significant?\n",
    "3. Do integrity dimensions correlate?\n",
    "4. Are there meaningful interactions between model \u00d7 constitution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.stats import f_oneway, ttest_ind, mannwhitneyu, kruskal, chi2_contingency\n",
    "from itertools import combinations\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load data\n",
    "EXPERIMENT_ID = \"exp_20251023_105245\"\n",
    "BASE_DIR = Path(\"..\")\n",
    "RESULTS_DIR = BASE_DIR / \"results\" / \"runs\" / EXPERIMENT_ID\n",
    "\n",
    "test_results = []\n",
    "for result_file in RESULTS_DIR.glob(\"*_result.json\"):\n",
    "    with open(result_file) as f:\n",
    "        result = json.load(f)\n",
    "        test_results.append({\n",
    "            'trial_id': result['trial_id'],\n",
    "            'scenario_id': result['scenario_id'],\n",
    "            'constitution_id': result['constitution_id'],\n",
    "            'model_id': result['model_id'],\n",
    "            'factual_adherence': result['integrity_evaluation']['factual_adherence'],\n",
    "            'value_transparency': result['integrity_evaluation']['value_transparency'],\n",
    "            'logical_coherence': result['integrity_evaluation']['logical_coherence'],\n",
    "            'overall_score': result['integrity_evaluation']['overall_integrity_score']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(test_results)\n",
    "print(f\"Loaded {len(df)} test results for statistical analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1: Model Differences Are Significant\n",
    "\n",
    "**Null Hypothesis (H0):** All models have the same mean integrity score  \n",
    "**Alternative (H1):** At least one model differs significantly\n",
    "\n",
    "We'll use ANOVA (parametric) and Kruskal-Wallis (non-parametric) tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normality assumption\n",
    "print(\"Normality Tests by Model (Shapiro-Wilk):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model in df['model_id'].unique():\n",
    "    model_scores = df[df['model_id'] == model]['overall_score']\n",
    "    stat, p = stats.shapiro(model_scores)\n",
    "    print(f\"{model:30s} p={p:.4f} {'(Normal)' if p > 0.05 else '(Non-normal)'}\")\n",
    "\n",
    "print(\"\\nNote: If p > 0.05, data is approximately normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA test\n",
    "model_groups = [df[df['model_id'] == model]['overall_score'].values for model in df['model_id'].unique()]\n",
    "f_stat, p_value_anova = f_oneway(*model_groups)\n",
    "\n",
    "print(\"One-Way ANOVA (Parametric Test):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_value_anova:.6f}\")\n",
    "print(f\"\\nResult: {'REJECT H0' if p_value_anova < 0.001 else 'FAIL TO REJECT H0'}\")\n",
    "print(f\"Interpretation: Model differences are {'statistically significant' if p_value_anova < 0.001 else 'not significant'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kruskal-Wallis test (non-parametric alternative)\n",
    "h_stat, p_value_kw = kruskal(*model_groups)\n",
    "\n",
    "print(\"Kruskal-Wallis Test (Non-Parametric):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"H-statistic: {h_stat:.4f}\")\n",
    "print(f\"p-value: {p_value_kw:.6f}\")\n",
    "print(f\"\\nResult: {'REJECT H0' if p_value_kw < 0.001 else 'FAIL TO REJECT H0'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise t-tests with Bonferroni correction\n",
    "models = df['model_id'].unique()\n",
    "n_comparisons = len(list(combinations(models, 2)))\n",
    "alpha_corrected = 0.05 / n_comparisons  # Bonferroni correction\n",
    "\n",
    "print(f\"Pairwise Model Comparisons (Bonferroni-corrected \u03b1 = {alpha_corrected:.4f})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pairwise_results = []\n",
    "\n",
    "for model1, model2 in combinations(models, 2):\n",
    "    scores1 = df[df['model_id'] == model1]['overall_score']\n",
    "    scores2 = df[df['model_id'] == model2]['overall_score']\n",
    "    \n",
    "    t_stat, p_value = ttest_ind(scores1, scores2)\n",
    "    mean_diff = scores1.mean() - scores2.mean()\n",
    "    \n",
    "    pairwise_results.append({\n",
    "        'Model 1': model1,\n",
    "        'Model 2': model2,\n",
    "        'Mean Diff': f\"{mean_diff:+.2f}\",\n",
    "        'p-value': f\"{p_value:.6f}\",\n",
    "        'Significant': 'Yes' if p_value < alpha_corrected else 'No'\n",
    "    })\n",
    "\n",
    "pairwise_df = pd.DataFrame(pairwise_results)\n",
    "pairwise_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 2: Bad-Faith vs. Honest Constitution Gap\n",
    "\n",
    "**Null Hypothesis (H0):** Bad-faith and honest constitutions have equal mean integrity  \n",
    "**Alternative (H1):** Bad-faith scores significantly lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "bad_faith_scores = df[df['constitution_id'] == 'bad-faith']['overall_score']\n",
    "honest_scores = df[df['constitution_id'] != 'bad-faith']['overall_score']\n",
    "\n",
    "print(\"Bad-Faith vs. Honest Constitutions:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Bad-Faith Mean: {bad_faith_scores.mean():.2f} (n={len(bad_faith_scores)})\")\n",
    "print(f\"Honest Mean: {honest_scores.mean():.2f} (n={len(honest_scores)})\")\n",
    "print(f\"Gap: {honest_scores.mean() - bad_faith_scores.mean():.2f} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-sample t-test\n",
    "t_stat, p_value_ttest = ttest_ind(honest_scores, bad_faith_scores, alternative='greater')\n",
    "\n",
    "print(\"\\nTwo-Sample t-Test (One-Tailed):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value_ttest:.10f}\")\n",
    "print(f\"\\nResult: {'REJECT H0' if p_value_ttest < 0.001 else 'FAIL TO REJECT H0'}\")\n",
    "print(f\"Interpretation: Honest constitutions score {'significantly higher' if p_value_ttest < 0.001 else 'not significantly higher'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Whitney U test (non-parametric)\n",
    "u_stat, p_value_mw = mannwhitneyu(honest_scores, bad_faith_scores, alternative='greater')\n",
    "\n",
    "print(\"Mann-Whitney U Test (Non-Parametric):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"U-statistic: {u_stat:.4f}\")\n",
    "print(f\"p-value: {p_value_mw:.10f}\")\n",
    "print(f\"\\nResult: {'REJECT H0' if p_value_mw < 0.001 else 'FAIL TO REJECT H0'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect size (Cohen's d)\n",
    "pooled_std = np.sqrt(((len(honest_scores) - 1) * honest_scores.std()**2 + \n",
    "                      (len(bad_faith_scores) - 1) * bad_faith_scores.std()**2) / \n",
    "                     (len(honest_scores) + len(bad_faith_scores) - 2))\n",
    "\n",
    "cohens_d = (honest_scores.mean() - bad_faith_scores.mean()) / pooled_std\n",
    "\n",
    "print(\"\\nEffect Size (Cohen's d):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"d = {cohens_d:.4f}\")\n",
    "print(f\"Interpretation: {'Small' if abs(cohens_d) < 0.5 else 'Medium' if abs(cohens_d) < 0.8 else 'Large'} effect size\")\n",
    "print(f\"\\nNote: d > 0.8 indicates a large practical difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3: Dimensional Failure in Bad-Faith\n",
    "\n",
    "Which integrity dimension is most affected by bad-faith reasoning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each dimension\n",
    "dimensions = ['factual_adherence', 'value_transparency', 'logical_coherence']\n",
    "\n",
    "print(\"Dimensional Differences (Bad-Faith vs. Honest):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dim in dimensions:\n",
    "    bad_faith_dim = df[df['constitution_id'] == 'bad-faith'][dim]\n",
    "    honest_dim = df[df['constitution_id'] != 'bad-faith'][dim]\n",
    "    \n",
    "    gap = honest_dim.mean() - bad_faith_dim.mean()\n",
    "    t_stat, p_value = ttest_ind(honest_dim, bad_faith_dim, alternative='greater')\n",
    "    \n",
    "    print(f\"\\n{dim.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Honest: {honest_dim.mean():.2f}\")\n",
    "    print(f\"  Bad-Faith: {bad_faith_dim.mean():.2f}\")\n",
    "    print(f\"  Gap: {gap:.2f} points\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    print(f\"  Significant: {'Yes' if p_value < 0.001 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "dim_data = {\n",
    "    'Factual\\nAdherence': [\n",
    "        df[df['constitution_id'] != 'bad-faith']['factual_adherence'].mean(),\n",
    "        df[df['constitution_id'] == 'bad-faith']['factual_adherence'].mean()\n",
    "    ],\n",
    "    'Value\\nTransparency': [\n",
    "        df[df['constitution_id'] != 'bad-faith']['value_transparency'].mean(),\n",
    "        df[df['constitution_id'] == 'bad-faith']['value_transparency'].mean()\n",
    "    ],\n",
    "    'Logical\\nCoherence': [\n",
    "        df[df['constitution_id'] != 'bad-faith']['logical_coherence'].mean(),\n",
    "        df[df['constitution_id'] == 'bad-faith']['logical_coherence'].mean()\n",
    "    ]\n",
    "}\n",
    "\n",
    "x = np.arange(len(dim_data))\n",
    "width = 0.35\n",
    "\n",
    "honest_means = [v[0] for v in dim_data.values()]\n",
    "bad_faith_means = [v[1] for v in dim_data.values()]\n",
    "\n",
    "ax.bar(x - width/2, honest_means, width, label='Honest', color='skyblue')\n",
    "ax.bar(x + width/2, bad_faith_means, width, label='Bad-Faith', color='salmon')\n",
    "\n",
    "ax.set_ylabel('Mean Score', fontweight='bold')\n",
    "ax.set_title('Dimensional Breakdown: Honest vs. Bad-Faith', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(dim_data.keys())\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 4: Honest Constitution Convergence\n",
    "\n",
    "Do honest constitutions (with different value frameworks) produce similar integrity scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to honest constitutions\n",
    "honest_consts = ['balanced-justice', 'harm-minimization', 'community-order', 'self-sovereignty']\n",
    "df_honest = df[df['constitution_id'].isin(honest_consts)]\n",
    "\n",
    "# ANOVA on honest constitutions only\n",
    "honest_groups = [df_honest[df_honest['constitution_id'] == const]['overall_score'].values \n",
    "                 for const in honest_consts]\n",
    "\n",
    "f_stat, p_value = f_oneway(*honest_groups)\n",
    "\n",
    "print(\"ANOVA: Honest Constitutions Only\")\n",
    "print(\"=\"*60)\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.6f}\")\n",
    "print(f\"\\nResult: {'Significant differences exist' if p_value < 0.05 else 'No significant differences'}\")\n",
    "print(\"\\nNote: Small p-value suggests constitutions differ, but the effect size matters more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spread\n",
    "const_means = df_honest.groupby('constitution_id')['overall_score'].mean()\n",
    "spread = const_means.max() - const_means.min()\n",
    "\n",
    "print(\"\\nHonest Constitution Spread:\")\n",
    "print(\"=\"*60)\n",
    "for const in honest_consts:\n",
    "    mean = const_means[const]\n",
    "    print(f\"{const:25s}: {mean:.2f}\")\n",
    "\n",
    "print(f\"\\nRange: {spread:.2f} points\")\n",
    "print(f\"\\nInterpretation: {'Tight clustering' if spread < 5 else 'Moderate spread' if spread < 10 else 'Wide spread'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 5: Model \u00d7 Constitution Interaction\n",
    "\n",
    "Do certain models handle bad-faith better than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degradation by model\n",
    "models = df['model_id'].unique()\n",
    "\n",
    "degradation_data = []\n",
    "\n",
    "for model in models:\n",
    "    honest_score = df[(df['model_id'] == model) & (df['constitution_id'] != 'bad-faith')]['overall_score'].mean()\n",
    "    bad_faith_score = df[(df['model_id'] == model) & (df['constitution_id'] == 'bad-faith')]['overall_score'].mean()\n",
    "    degradation = honest_score - bad_faith_score\n",
    "    \n",
    "    degradation_data.append({\n",
    "        'Model': model,\n",
    "        'Honest Mean': honest_score,\n",
    "        'Bad-Faith Mean': bad_faith_score,\n",
    "        'Degradation': degradation,\n",
    "        'Degradation %': (degradation / honest_score * 100)\n",
    "    })\n",
    "\n",
    "degradation_df = pd.DataFrame(degradation_data).sort_values('Degradation', ascending=False)\n",
    "\n",
    "print(\"Bad-Faith Degradation by Model:\")\n",
    "print(\"=\"*80)\n",
    "degradation_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(degradation_df))\n",
    "ax.bar(x, degradation_df['Degradation'], color='coral', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold')\n",
    "ax.set_ylabel('Integrity Degradation (points)', fontweight='bold')\n",
    "ax.set_title('Bad-Faith Degradation by Model\\n(Higher = More Susceptible to Motivated Reasoning)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(degradation_df['Model'], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Statistical Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. MODEL DIFFERENCES:\")\n",
    "print(f\"   - ANOVA p-value: {p_value_anova:.6f} \u2192 Models differ significantly\")\n",
    "print(f\"   - Top model significantly outperforms bottom model (p < 0.001)\")\n",
    "\n",
    "print(\"\\n2. MOTIVATED REASONING DETECTION:\")\n",
    "print(f\"   - t-test p-value: {p_value_ttest:.10f} \u2192 Bad-faith scores significantly lower\")\n",
    "print(f\"   - Effect size (Cohen's d): {cohens_d:.4f} \u2192 Large practical difference\")\n",
    "print(f\"   - Mean gap: {honest_scores.mean() - bad_faith_scores.mean():.2f} points\")\n",
    "\n",
    "print(\"\\n3. DIMENSIONAL BREAKDOWN:\")\n",
    "bad_faith_factual = df[df['constitution_id'] == 'bad-faith']['factual_adherence'].mean()\n",
    "honest_factual = df[df['constitution_id'] != 'bad-faith']['factual_adherence'].mean()\n",
    "factual_gap = honest_factual - bad_faith_factual\n",
    "print(f\"   - Factual adherence most affected: {factual_gap:.2f} point gap\")\n",
    "print(f\"   - Bad-faith primarily degrades factual accuracy, not value statements\")\n",
    "\n",
    "print(\"\\n4. VALUE PLURALISM:\")\n",
    "print(f\"   - Honest constitution spread: {spread:.2f} points\")\n",
    "print(f\"   - Interpretation: Different value frameworks maintain similar integrity\")\n",
    "\n",
    "print(\"\\n5. MODEL ROBUSTNESS:\")\n",
    "most_robust = degradation_df.iloc[-1]['Model']\n",
    "least_robust = degradation_df.iloc[0]['Model']\n",
    "print(f\"   - Most robust to bad-faith: {most_robust}\")\n",
    "print(f\"   - Least robust: {least_robust}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Constitution Project",
   "language": "python",
   "name": "constitution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}