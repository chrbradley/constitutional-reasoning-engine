{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitutional Reasoning Experiment - Exploratory Analysis\n",
    "\n",
    "**Experiment ID:** exp_20251023_105245  \n",
    "**Tests Completed:** 480/480 (100%)  \n",
    "**Configuration:** 16 scenarios \u00d7 5 constitutions \u00d7 6 models\n",
    "\n",
    "This notebook provides interactive exploration of the experiment results, allowing deep-dive analysis beyond the automated reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Constants\n",
    "EXPERIMENT_ID = \"exp_20251023_105245\"\n",
    "BASE_DIR = Path(\"..\")  # Assuming notebook is in notebooks/ directory\n",
    "RESULTS_DIR = BASE_DIR / \"results\" / \"runs\" / EXPERIMENT_ID\n",
    "ANALYSIS_PATH = BASE_DIR / \"results\" / \"analysis\" / \"single\" / f\"{EXPERIMENT_ID}_analysis.json\"\n",
    "\n",
    "print(f\"Loading experiment: {EXPERIMENT_ID}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analysis results\n",
    "with open(ANALYSIS_PATH) as f:\n",
    "    analysis = json.load(f)\n",
    "\n",
    "print(f\"Analysis timestamp: {analysis['analysis_timestamp']}\")\n",
    "print(f\"Total tests: {analysis['total_tests']}\")\n",
    "print(f\"Overall mean score: {analysis['summary']['overall_score']['mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw test results into DataFrame\n",
    "test_results = []\n",
    "\n",
    "for result_file in RESULTS_DIR.glob(\"*_result.json\"):\n",
    "    with open(result_file) as f:\n",
    "        result = json.load(f)\n",
    "        \n",
    "        test_results.append({\n",
    "            'trial_id': result['trial_id'],\n",
    "            'scenario_id': result['scenario_id'],\n",
    "            'constitution_id': result['constitution_id'],\n",
    "            'model_id': result['model_id'],\n",
    "            'factual_adherence': result['integrity_evaluation']['factual_adherence'],\n",
    "            'value_transparency': result['integrity_evaluation']['value_transparency'],\n",
    "            'logical_coherence': result['integrity_evaluation']['logical_coherence'],\n",
    "            'overall_score': result['integrity_evaluation']['overall_integrity_score'],\n",
    "            'timestamp': result['timestamp']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(test_results)\n",
    "\n",
    "print(f\"Loaded {len(df)} test results\")\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Overall Score Statistics:\")\n",
    "print(df['overall_score'].describe())\n",
    "\n",
    "print(\"\\nIntegrity Dimension Statistics:\")\n",
    "print(df[['factual_adherence', 'value_transparency', 'logical_coherence']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Overall score distribution\n",
    "axes[0, 0].hist(df['overall_score'], bins=20, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(df['overall_score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"overall_score\"].mean():.1f}')\n",
    "axes[0, 0].axvline(df['overall_score'].median(), color='green', linestyle='--', label=f'Median: {df[\"overall_score\"].median():.1f}')\n",
    "axes[0, 0].set_xlabel('Overall Integrity Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Overall Score Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Factual adherence\n",
    "axes[0, 1].hist(df['factual_adherence'], bins=20, color='steelblue', edgecolor='black')\n",
    "axes[0, 1].axvline(df['factual_adherence'].mean(), color='red', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Factual Adherence Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Factual Adherence Distribution')\n",
    "\n",
    "# Value transparency\n",
    "axes[1, 0].hist(df['value_transparency'], bins=20, color='seagreen', edgecolor='black')\n",
    "axes[1, 0].axvline(df['value_transparency'].mean(), color='red', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Value Transparency Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Value Transparency Distribution')\n",
    "\n",
    "# Logical coherence\n",
    "axes[1, 1].hist(df['logical_coherence'], bins=20, color='coral', edgecolor='black')\n",
    "axes[1, 1].axvline(df['logical_coherence'].mean(), color='red', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Logical Coherence Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Logical Coherence Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model\n",
    "model_stats = df.groupby('model_id').agg({\n",
    "    'overall_score': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "    'factual_adherence': 'mean',\n",
    "    'value_transparency': 'mean',\n",
    "    'logical_coherence': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "model_stats.columns = ['_'.join(col).strip() for col in model_stats.columns.values]\n",
    "model_stats = model_stats.sort_values('overall_score_mean', ascending=False)\n",
    "\n",
    "print(\"Model Performance Rankings:\")\n",
    "model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot: Score distributions by model\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "models_sorted = model_stats.index.tolist()\n",
    "df['model_id_cat'] = pd.Categorical(df['model_id'], categories=models_sorted, ordered=True)\n",
    "\n",
    "sns.boxplot(data=df, x='model_id_cat', y='overall_score', ax=ax)\n",
    "ax.set_xlabel('Model', fontweight='bold')\n",
    "ax.set_ylabel('Integrity Score', fontweight='bold')\n",
    "ax.set_title('Score Distributions by Model (Box Plot)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.axhline(80, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constitution Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by constitution\n",
    "const_stats = df.groupby('constitution_id').agg({\n",
    "    'overall_score': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "    'factual_adherence': 'mean',\n",
    "    'value_transparency': 'mean',\n",
    "    'logical_coherence': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "const_stats.columns = ['_'.join(col).strip() for col in const_stats.columns.values]\n",
    "const_stats = const_stats.sort_values('overall_score_mean', ascending=False)\n",
    "\n",
    "print(\"Constitution Performance Rankings:\")\n",
    "const_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot: Score distributions by constitution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "consts_sorted = const_stats.index.tolist()\n",
    "df['constitution_id_cat'] = pd.Categorical(df['constitution_id'], categories=consts_sorted, ordered=True)\n",
    "\n",
    "box_colors = ['salmon' if c == 'bad-faith' else 'skyblue' for c in consts_sorted]\n",
    "\n",
    "bp = sns.boxplot(data=df, x='constitution_id_cat', y='overall_score', ax=ax)\n",
    "for patch, color in zip(bp.patches if hasattr(bp, 'patches') else bp.artists, box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_xlabel('Constitution', fontweight='bold')\n",
    "ax.set_ylabel('Integrity Score', fontweight='bold')\n",
    "ax.set_title('Score Distributions by Constitution (Bad-Faith Highlighted)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.axhline(80, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \u00d7 Constitution Interaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table: Model \u00d7 Constitution mean scores\n",
    "pivot = df.pivot_table(\n",
    "    values='overall_score',\n",
    "    index='model_id',\n",
    "    columns='constitution_id',\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "\n",
    "# Reorder by model performance\n",
    "pivot = pivot.loc[models_sorted]\n",
    "\n",
    "print(\"Model \u00d7 Constitution Interaction Matrix (Mean Scores):\")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(pivot, annot=True, fmt='.1f', cmap='RdYlGn', vmin=40, vmax=95,\n",
    "            cbar_kws={'label': 'Integrity Score'}, ax=ax)\n",
    "\n",
    "ax.set_title('Model \u00d7 Constitution Interaction Heatmap', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Constitution', fontweight='bold')\n",
    "ax.set_ylabel('Model', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario Difficulty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by scenario\n",
    "scenario_stats = df.groupby('scenario_id').agg({\n",
    "    'overall_score': ['mean', 'std', 'min', 'max', 'count']\n",
    "}).round(2)\n",
    "\n",
    "scenario_stats.columns = ['_'.join(col).strip() for col in scenario_stats.columns.values]\n",
    "scenario_stats = scenario_stats.sort_values('overall_score_mean')\n",
    "\n",
    "print(\"Scenario Difficulty Rankings (Lowest Score = Hardest):\")\n",
    "scenario_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Mean vs. Variability\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    scenario_stats['overall_score_mean'],\n",
    "    scenario_stats['overall_score_std'],\n",
    "    s=150,\n",
    "    alpha=0.6,\n",
    "    c=scenario_stats['overall_score_mean'],\n",
    "    cmap='RdYlGn',\n",
    "    vmin=70,\n",
    "    vmax=90\n",
    ")\n",
    "\n",
    "# Label interesting points\n",
    "for scenario_id in scenario_stats.index:\n",
    "    mean = scenario_stats.loc[scenario_id, 'overall_score_mean']\n",
    "    std = scenario_stats.loc[scenario_id, 'overall_score_std']\n",
    "    \n",
    "    if mean < 78 or std > 20:\n",
    "        ax.annotate(\n",
    "            scenario_id.replace('-', ' ').title(),\n",
    "            (mean, std),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=9,\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "ax.set_xlabel('Mean Integrity Score', fontweight='bold')\n",
    "ax.set_ylabel('Standard Deviation', fontweight='bold')\n",
    "ax.set_title('Scenario Difficulty Analysis\\n(Lower-left = Harder, Upper-right = More Variable)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Mean Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df[['factual_adherence', 'value_transparency', 'logical_coherence', 'overall_score']].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={'label': 'Correlation'}, ax=ax)\n",
    "\n",
    "ax.set_title('Integrity Dimension Correlation Matrix', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "dimensions = ['factual_adherence', 'value_transparency', 'logical_coherence']\n",
    "sns.pairplot(df[dimensions], diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('Integrity Dimensions Pairplot', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive: Bad-Faith vs. Honest Constitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "df_bad_faith = df[df['constitution_id'] == 'bad-faith']\n",
    "df_honest = df[df['constitution_id'] != 'bad-faith']\n",
    "\n",
    "print(\"Bad-Faith Statistics:\")\n",
    "print(df_bad_faith['overall_score'].describe())\n",
    "\n",
    "print(\"\\nHonest Constitutions Statistics:\")\n",
    "print(df_honest['overall_score'].describe())\n",
    "\n",
    "print(f\"\\nMean Gap: {df_honest['overall_score'].mean() - df_bad_faith['overall_score'].mean():.2f} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison violin plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Overall scores\n",
    "comparison_df = pd.concat([\n",
    "    df_bad_faith.assign(group='Bad-Faith'),\n",
    "    df_honest.assign(group='Honest')\n",
    "])\n",
    "\n",
    "sns.violinplot(data=comparison_df, x='group', y='overall_score', ax=axes[0])\n",
    "axes[0].set_xlabel('Constitution Type', fontweight='bold')\n",
    "axes[0].set_ylabel('Overall Integrity Score', fontweight='bold')\n",
    "axes[0].set_title('Bad-Faith vs. Honest Constitutions', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Dimensional breakdown\n",
    "dim_comparison = pd.DataFrame({\n",
    "    'Bad-Faith': [\n",
    "        df_bad_faith['factual_adherence'].mean(),\n",
    "        df_bad_faith['value_transparency'].mean(),\n",
    "        df_bad_faith['logical_coherence'].mean()\n",
    "    ],\n",
    "    'Honest': [\n",
    "        df_honest['factual_adherence'].mean(),\n",
    "        df_honest['value_transparency'].mean(),\n",
    "        df_honest['logical_coherence'].mean()\n",
    "    ]\n",
    "}, index=['Factual\\nAdherence', 'Value\\nTransparency', 'Logical\\nCoherence'])\n",
    "\n",
    "dim_comparison.plot(kind='bar', ax=axes[1], color=['salmon', 'skyblue'])\n",
    "axes[1].set_xlabel('Dimension', fontweight='bold')\n",
    "axes[1].set_ylabel('Mean Score', fontweight='bold')\n",
    "axes[1].set_title('Dimensional Breakdown Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "axes[1].legend(title='Type')\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"KEY INSIGHTS FROM EXPLORATORY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Top model\n",
    "top_model = model_stats.index[0]\n",
    "top_score = model_stats.iloc[0]['overall_score_mean']\n",
    "print(f\"\\n1. TOP MODEL: {top_model} ({top_score:.1f}/100)\")\n",
    "\n",
    "# Motivated reasoning gap\n",
    "honest_mean = df_honest['overall_score'].mean()\n",
    "bad_faith_mean = df_bad_faith['overall_score'].mean()\n",
    "gap = honest_mean - bad_faith_mean\n",
    "print(f\"\\n2. MOTIVATED REASONING DETECTION: {gap:.1f} point gap\")\n",
    "print(f\"   - Honest constitutions: {honest_mean:.1f}/100\")\n",
    "print(f\"   - Bad-faith constitution: {bad_faith_mean:.1f}/100\")\n",
    "\n",
    "# Value pluralism\n",
    "honest_consts = const_stats[const_stats.index != 'bad-faith']\n",
    "spread = honest_consts['overall_score_mean'].max() - honest_consts['overall_score_mean'].min()\n",
    "print(f\"\\n3. VALUE PLURALISM: Only {spread:.1f} point spread among honest constitutions\")\n",
    "print(f\"   - Validates: Different values \u2260 Different facts\")\n",
    "\n",
    "# Hardest scenario\n",
    "hardest = scenario_stats.index[0]\n",
    "hardest_score = scenario_stats.iloc[0]['overall_score_mean']\n",
    "print(f\"\\n4. HARDEST SCENARIO: {hardest} ({hardest_score:.1f}/100)\")\n",
    "\n",
    "# Most variable\n",
    "most_variable_idx = scenario_stats['overall_score_std'].argmax()\n",
    "most_variable = scenario_stats.index[most_variable_idx]\n",
    "variability = scenario_stats.iloc[most_variable_idx]['overall_score_std']\n",
    "print(f\"\\n5. MOST VARIABLE SCENARIO: {most_variable} (SD: {variability:.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Constitution Project",
   "language": "python",
   "name": "constitution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}