{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Rater Reliability Analysis: Evaluator Agreement Patterns\n",
    "\n",
    "**Week 1, Task 2 (Analysis 1.3) of Analysis & Publication Plan**\n",
    "\n",
    "**Research Question:** Which evaluators show reliable agreement? Should we exclude any outliers?\n",
    "\n",
    "**Dataset:** 360 trials × 5 evaluators = 1,800 Likert evaluations\n",
    "\n",
    "**Evaluators Tested:**\n",
    "- Claude Sonnet 4.5 (claude-sonnet-4-5)\n",
    "- GPT-4o (gpt-4o)\n",
    "- Gemini 2.5 Pro (gemini-2-5-pro)\n",
    "- Grok 3 (grok-3)\n",
    "- DeepSeek Chat (deepseek-chat)\n",
    "\n",
    "**Purpose:** \n",
    "1. Identify outlier evaluators (if any)\n",
    "2. Generate consensus scores for downstream analyses\n",
    "3. Identify high-disagreement trials for human validation\n",
    "4. Understand reliability by constitution/scenario (stratified analysis)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from analysis.evaluator_agreement import EvaluatorAgreementAnalyzer\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Experiment ID\n",
    "EXPERIMENT_ID = 'exp_20251028_134615'\n",
    "\n",
    "print(\"✅ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Run Analysis\n",
    "\n",
    "This uses the `evaluator_agreement.py` script to calculate:\n",
    "- Pairwise correlations (Pearson r) between all 5 evaluators\n",
    "- Intraclass Correlation Coefficient (ICC) for agreement\n",
    "- Outlier evaluator detection (threshold: r < 0.50)\n",
    "- Consensus scores (mean, median, trimmed mean)\n",
    "- Stratified reliability by constitution and scenario\n",
    "- High-disagreement trials (top 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete analysis\n",
    "analyzer = EvaluatorAgreementAnalyzer(EXPERIMENT_ID)\n",
    "results = analyzer.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Overall Inter-Rater Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract overall reliability metrics\n",
    "reliability_summary = []\n",
    "\n",
    "for dimension in ['epistemic_integrity', 'value_transparency', 'overall_score']:\n",
    "    dim_data = results['dimensions'][dimension]\n",
    "    \n",
    "    reliability_summary.append({\n",
    "        'Dimension': dimension.replace('_', ' ').title(),\n",
    "        'Mean r': f\"{dim_data['pairwise_correlations']['mean_r']:.3f}\",\n",
    "        'Std r': f\"{dim_data['pairwise_correlations']['std_r']:.3f}\",\n",
    "        'Range': f\"[{dim_data['pairwise_correlations']['min_r']:.3f}, {dim_data['pairwise_correlations']['max_r']:.3f}]\",\n",
    "        'ICC(2,1)': f\"{dim_data['icc']['icc_single']:.3f}\",\n",
    "        'ICC(2,k)': f\"{dim_data['icc']['icc_average']:.3f}\"\n",
    "    })\n",
    "\n",
    "df_reliability = pd.DataFrame(reliability_summary)\n",
    "df_reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Pairwise Correlation Heatmap\n",
    "\n",
    "**Interpretation:**\n",
    "- Darker colors = higher correlation (better agreement)\n",
    "- Look for outliers: evaluators with consistently low correlations\n",
    "- Target: r > 0.60 (moderate reliability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmaps for each dimension\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "evaluators = results['evaluators']\n",
    "dimensions = ['epistemic_integrity', 'value_transparency', 'overall_score']\n",
    "dimension_names = ['Epistemic Integrity', 'Value Transparency', 'Overall Score']\n",
    "\n",
    "for idx, (dimension, dimension_name) in enumerate(zip(dimensions, dimension_names)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Build correlation matrix\n",
    "    n_evaluators = len(evaluators)\n",
    "    corr_matrix = np.ones((n_evaluators, n_evaluators))\n",
    "    \n",
    "    all_pairs = results['dimensions'][dimension]['pairwise_correlations']['all_pairs']\n",
    "    \n",
    "    for i in range(n_evaluators):\n",
    "        for j in range(n_evaluators):\n",
    "            if i == j:\n",
    "                corr_matrix[i, j] = 1.0\n",
    "            elif i < j:\n",
    "                eval1 = evaluators[i]\n",
    "                eval2 = evaluators[j]\n",
    "                pair_key = f\"{eval1}_vs_{eval2}\"\n",
    "                \n",
    "                if pair_key in all_pairs:\n",
    "                    corr_matrix[i, j] = all_pairs[pair_key]['r']\n",
    "                    corr_matrix[j, i] = all_pairs[pair_key]['r']\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='RdYlGn',\n",
    "        vmin=0,\n",
    "        vmax=1.0,\n",
    "        center=0.50,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'label': 'Pearson r'},\n",
    "        ax=ax,\n",
    "        xticklabels=[e.replace('claude-sonnet-4-5', 'Claude').replace('gpt-4o', 'GPT-4o').replace('gemini-2-5-pro', 'Gemini').replace('grok-3', 'Grok').replace('deepseek-chat', 'DeepSeek') for e in evaluators],\n",
    "        yticklabels=[e.replace('claude-sonnet-4-5', 'Claude').replace('gpt-4o', 'GPT-4o').replace('gemini-2-5-pro', 'Gemini').replace('grok-3', 'Grok').replace('deepseek-chat', 'DeepSeek') for e in evaluators]\n",
    "    )\n",
    "    \n",
    "    ax.set_title(dimension_name, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Evaluator Pairwise Correlations (Pearson r)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Look for dark red cells (high agreement) and yellow/green cells (low agreement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Evaluator Agreement Rankings\n",
    "\n",
    "**Question:** Which evaluators show strongest agreement with others?\n",
    "\n",
    "Shows mean correlation for each evaluator with all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection summary\n",
    "outlier_data = []\n",
    "\n",
    "for dimension in ['epistemic_integrity', 'value_transparency', 'overall_score']:\n",
    "    outlier_stats = results['dimensions'][dimension]['outlier_detection']\n",
    "    \n",
    "    for evaluator, stats_dict in outlier_stats.items():\n",
    "        if not np.isnan(stats_dict['mean_r']):\n",
    "            outlier_data.append({\n",
    "                'Dimension': dimension.replace('_', ' ').title(),\n",
    "                'Evaluator': evaluator.replace('claude-sonnet-4-5', 'Claude').replace('gpt-4o', 'GPT-4o').replace('gemini-2-5-pro', 'Gemini').replace('grok-3', 'Grok').replace('deepseek-chat', 'DeepSeek'),\n",
    "                'Mean r': stats_dict['mean_r'],\n",
    "                'Z-score': stats_dict['z_score'] if stats_dict['z_score'] is not None else 0,\n",
    "                'Is Outlier': stats_dict['is_outlier']\n",
    "            })\n",
    "\n",
    "df_outlier = pd.DataFrame(outlier_data)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, dimension_name in enumerate(['Epistemic Integrity', 'Value Transparency', 'Overall Score']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Filter data for this dimension\n",
    "    dim_data = df_outlier[df_outlier['Dimension'] == dimension_name].sort_values('Mean r', ascending=True)\n",
    "    \n",
    "    # Color bars: red if outlier, blue otherwise\n",
    "    colors = ['red' if is_outlier else '#1f77b4' for is_outlier in dim_data['Is Outlier']]\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    bars = ax.barh(\n",
    "        dim_data['Evaluator'],\n",
    "        dim_data['Mean r'],\n",
    "        color=colors,\n",
    "        edgecolor='black',\n",
    "        linewidth=1.5,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean_r in zip(bars, dim_data['Mean r']):\n",
    "        width = bar.get_width()\n",
    "        ax.text(\n",
    "            width + 0.01,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f'{mean_r:.3f}',\n",
    "            ha='left',\n",
    "            va='center',\n",
    "            fontweight='bold',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    # Reference line at threshold\n",
    "    ax.axvline(x=0.50, color='orange', linestyle='--', alpha=0.7, linewidth=2, label='Outlier threshold (r=0.50)')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_title(dimension_name, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Mean Correlation with Others (Pearson r)', fontsize=11)\n",
    "    ax.set_xlim(0, 0.80)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    if idx == 0:\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "\n",
    "plt.suptitle('Evaluator Agreement Rankings (Mean r with Others)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Red bars indicate outlier evaluators (mean r < 0.50)\")\n",
    "print(\"📊 Higher values = better agreement with other evaluators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Evaluator Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any outliers detected\n",
    "outliers_found = df_outlier[df_outlier['Is Outlier'] == True]\n",
    "\n",
    "if len(outliers_found) > 0:\n",
    "    print(\"⚠️ OUTLIER EVALUATORS DETECTED:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for _, row in outliers_found.iterrows():\n",
    "        print(f\"\\n  {row['Evaluator']} ({row['Dimension']})\")\n",
    "        print(f\"    Mean r with others: {row['Mean r']:.3f}\")\n",
    "        print(f\"    Z-score: {row['Z-score']:+.2f}\")\n",
    "        print(f\"    Status: Below threshold (r < 0.50)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\\n⚠️ RECOMMENDATION: Consider excluding outlier evaluator(s) from consensus scores.\")\n",
    "    print(\"   Consensus scores will be generated with both:\")\n",
    "    print(\"   - All 5 evaluators (mean_all)\")\n",
    "    print(\"   - Excluding outlier (mean_excluding_outlier)\")\n",
    "    \n",
    "else:\n",
    "    print(\"✅ NO OUTLIERS DETECTED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nAll evaluators show acceptable agreement (mean r ≥ 0.50 with others).\")\n",
    "    print(\"\\n✅ RECOMMENDATION: Use all 5 evaluators for consensus scores.\")\n",
    "\n",
    "# Show full outlier stats table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FULL EVALUATOR AGREEMENT TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df_outlier_pivot = df_outlier.pivot(index='Evaluator', columns='Dimension', values='Mean r')\n",
    "df_outlier_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Reliability Analysis\n",
    "\n",
    "**Question:** Does inter-rater reliability vary by:\n",
    "- Constitution (do certain value systems cause more disagreement?)\n",
    "- Scenario (do certain topics cause more disagreement?)\n",
    "\n",
    "**Goal:** Identify problematic subgroups before human validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stratified reliability data\n",
    "stratified_data_const = []\n",
    "stratified_data_scenario = []\n",
    "\n",
    "dimension = 'overall_score'  # Use overall score for stratified analysis\n",
    "stratified = results['dimensions'][dimension]['stratified_reliability']\n",
    "\n",
    "# By constitution\n",
    "for constitution, stats_dict in stratified['by_constitution'].items():\n",
    "    stratified_data_const.append({\n",
    "        'Constitution': constitution,\n",
    "        'n Trials': stats_dict['n_trials'],\n",
    "        'Mean r': stats_dict['mean_r'],\n",
    "        'ICC(2,1)': stats_dict['icc_single']\n",
    "    })\n",
    "\n",
    "df_stratified_const = pd.DataFrame(stratified_data_const).sort_values('Mean r')\n",
    "\n",
    "# By scenario\n",
    "for scenario, stats_dict in stratified['by_scenario'].items():\n",
    "    stratified_data_scenario.append({\n",
    "        'Scenario': scenario,\n",
    "        'n Trials': stats_dict['n_trials'],\n",
    "        'Mean r': stats_dict['mean_r'],\n",
    "        'ICC(2,1)': stats_dict['icc_single']\n",
    "    })\n",
    "\n",
    "df_stratified_scenario = pd.DataFrame(stratified_data_scenario).sort_values('Mean r')\n",
    "\n",
    "print(\"Stratified reliability data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Stratified reliability by constitution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# By Constitution\n",
    "ax = axes[0]\n",
    "bars = ax.barh(\n",
    "    df_stratified_const['Constitution'],\n",
    "    df_stratified_const['Mean r'],\n",
    "    color='#1f77b4',\n",
    "    edgecolor='black',\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add value labels and sample sizes\n",
    "for bar, mean_r, n_trials in zip(bars, df_stratified_const['Mean r'], df_stratified_const['n Trials']):\n",
    "    width = bar.get_width()\n",
    "    ax.text(\n",
    "        width + 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f'{mean_r:.3f} (n={n_trials})',\n",
    "        ha='left',\n",
    "        va='center',\n",
    "        fontweight='bold',\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "ax.axvline(x=0.40, color='orange', linestyle='--', alpha=0.7, linewidth=2, label='Overall mean r')\n",
    "ax.set_title('Inter-Rater Reliability by Constitution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Mean Pearson r (Overall Score)', fontsize=11)\n",
    "ax.set_xlim(0, 0.60)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.legend(fontsize=9, loc='lower right')\n",
    "\n",
    "# By Scenario\n",
    "ax = axes[1]\n",
    "bars = ax.barh(\n",
    "    df_stratified_scenario['Scenario'],\n",
    "    df_stratified_scenario['Mean r'],\n",
    "    color='#2ca02c',\n",
    "    edgecolor='black',\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add value labels and sample sizes\n",
    "for bar, mean_r, n_trials in zip(bars, df_stratified_scenario['Mean r'], df_stratified_scenario['n Trials']):\n",
    "    width = bar.get_width()\n",
    "    ax.text(\n",
    "        width + 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f'{mean_r:.3f} (n={n_trials})',\n",
    "        ha='left',\n",
    "        va='center',\n",
    "        fontweight='bold',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "ax.axvline(x=0.40, color='orange', linestyle='--', alpha=0.7, linewidth=2, label='Overall mean r')\n",
    "ax.set_title('Inter-Rater Reliability by Scenario', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Mean Pearson r (Overall Score)', fontsize=11)\n",
    "ax.set_xlim(0, 0.60)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.legend(fontsize=9, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Look for bars significantly below the overall mean (orange line) - these subgroups have lower reliability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Analysis Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify problematic constitutions/scenarios\n",
    "overall_mean_r = results['dimensions']['overall_score']['pairwise_correlations']['mean_r']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"OVERALL MEAN RELIABILITY: r = {overall_mean_r:.3f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Constitutions below mean\n",
    "problematic_const = df_stratified_const[df_stratified_const['Mean r'] < overall_mean_r - 0.05]\n",
    "if len(problematic_const) > 0:\n",
    "    print(\"\\n⚠️ CONSTITUTIONS WITH LOWER RELIABILITY (>0.05 below mean):\")\n",
    "    for _, row in problematic_const.iterrows():\n",
    "        print(f\"   - {row['Constitution']:25} → r={row['Mean r']:.3f} (n={row['n Trials']})\")\n",
    "else:\n",
    "    print(\"\\n✅ All constitutions show comparable reliability (within 0.05 of mean)\")\n",
    "\n",
    "# Scenarios below mean\n",
    "problematic_scenario = df_stratified_scenario[df_stratified_scenario['Mean r'] < overall_mean_r - 0.05]\n",
    "if len(problematic_scenario) > 0:\n",
    "    print(\"\\n⚠️ SCENARIOS WITH LOWER RELIABILITY (>0.05 below mean):\")\n",
    "    for _, row in problematic_scenario.iterrows():\n",
    "        print(f\"   - {row['Scenario']:35} → r={row['Mean r']:.3f} (n={row['n Trials']})\")\n",
    "else:\n",
    "    print(\"\\n✅ All scenarios show comparable reliability (within 0.05 of mean)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPLICATIONS FOR HUMAN VALIDATION (Week 2-3):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(problematic_const) > 0 or len(problematic_scenario) > 0:\n",
    "    print(\"\\n⚠️ Prioritize these subgroups for manual review:\")\n",
    "    if len(problematic_const) > 0:\n",
    "        print(\"   - Constitutions:\", \", \".join(problematic_const['Constitution'].tolist()))\n",
    "    if len(problematic_scenario) > 0:\n",
    "        print(\"   - Scenarios:\", \", \".join([s[:40] for s in problematic_scenario['Scenario'].tolist()]))\n",
    "    print(\"\\n   These subgroups show higher evaluator disagreement and may benefit\")\n",
    "    print(\"   from additional human validation to establish ground truth.\")\n",
    "else:\n",
    "    print(\"\\n✅ Reliability is consistent across all constitutions and scenarios.\")\n",
    "    print(\"   Stratified sampling can be uniform - no need to oversample specific subgroups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Disagreement Trials\n",
    "\n",
    "**Question:** Which trials show highest evaluator disagreement?\n",
    "\n",
    "**Purpose:** These trials are candidates for priority review in human validation.\n",
    "\n",
    "**Metric:** Standard deviation across evaluators (higher = more disagreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract high-disagreement trials\n",
    "high_disagreement = results['high_disagreement_trials']\n",
    "\n",
    "print(f\"High-Disagreement Trials Identified: {len(high_disagreement)} (top 10%)\")\n",
    "print(\"\\nTop 20 trials with highest evaluator disagreement:\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for i, trial_info in enumerate(high_disagreement[:20], 1):\n",
    "    print(f\"{i:2}. {trial_info['trial_id']:20} → Max SD: {trial_info['max_disagreement']:5.2f}\")\n",
    "    print(f\"    Epistemic Integrity SD: {trial_info['std_epistemic_integrity']:5.2f}\")\n",
    "    print(f\"    Value Transparency SD:  {trial_info['std_value_transparency']:5.2f}\")\n",
    "    print(f\"    Overall Score SD:       {trial_info['std_overall_score']:5.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"\\n📋 RECOMMENDATION FOR WEEK 2 VALIDATION:\")\n",
    "print(\"   - Include 5-10 high-disagreement trials in validation sample\")\n",
    "print(\"   - Manually review to understand why evaluators disagreed\")\n",
    "print(\"   - May reveal rubric ambiguities or edge cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus Scores Generation\n",
    "\n",
    "**Purpose:** Create consolidated scores for downstream analyses (Weeks 1-4)\n",
    "\n",
    "**Methods:**\n",
    "1. **Mean (all 5):** Simple average across all evaluators\n",
    "2. **Median:** Robust to outliers\n",
    "3. **Trimmed mean:** Remove highest/lowest, average remaining 3\n",
    "4. **Mean (excluding outlier):** If outlier detected, compute mean of remaining 4\n",
    "\n",
    "**Recommendation:** Will be determined based on outlier detection above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consensus scores already generated by analyzer\n",
    "consensus_scores = results['consensus_scores']\n",
    "\n",
    "print(f\"✅ Consensus scores generated for {len(consensus_scores)} trials\")\n",
    "print(\"\\nConsensus methods available:\")\n",
    "print(\"  1. mean_all: Mean across all 5 evaluators\")\n",
    "print(\"  2. median_all: Median across all 5 evaluators\")\n",
    "print(\"  3. trimmed_mean: Mean of middle 3 (removes highest/lowest)\")\n",
    "\n",
    "# Check if outlier was detected\n",
    "outlier_evaluator = None\n",
    "for dimension in ['epistemic_integrity', 'value_transparency', 'overall_score']:\n",
    "    outlier_stats = results['dimensions'][dimension]['outlier_detection']\n",
    "    for evaluator, stats_dict in outlier_stats.items():\n",
    "        if stats_dict['is_outlier']:\n",
    "            outlier_evaluator = evaluator\n",
    "            break\n",
    "    if outlier_evaluator:\n",
    "        break\n",
    "\n",
    "if outlier_evaluator:\n",
    "    print(f\"  4. mean_excluding_outlier: Mean excluding {outlier_evaluator}\")\n",
    "else:\n",
    "    print(\"  (No outlier detected, mean_excluding_outlier = mean_all)\")\n",
    "\n",
    "# Example consensus score\n",
    "print(\"\\nExample consensus score (first trial):\")\n",
    "print(json.dumps(consensus_scores[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision: Which Consensus Method to Use?\n",
    "\n",
    "**Criteria:**\n",
    "- If no outlier detected → Use **mean_all** (simplest, uses all data)\n",
    "- If outlier detected → Use **mean_excluding_outlier** (removes systematic bias)\n",
    "- **Trimmed mean** is robust alternative (always removes extreme values)\n",
    "- **Median** is most robust but loses information (use if high variance)\n",
    "\n",
    "**Recommendation based on this analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make recommendation\n",
    "print(\"=\" * 70)\n",
    "print(\"CONSENSUS SCORING RECOMMENDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if outlier_evaluator:\n",
    "    print(f\"\\n⚠️ OUTLIER DETECTED: {outlier_evaluator}\")\n",
    "    print(\"\\n✅ RECOMMENDATION: Use mean_excluding_outlier for downstream analyses\")\n",
    "    print(\"\\n   Rationale:\")\n",
    "    print(f\"   - {outlier_evaluator} shows systematically low correlation with other evaluators\")\n",
    "    print(\"   - Excluding improves reliability and reduces bias\")\n",
    "    print(\"   - Still have 4 evaluators (sufficient for consensus)\")\n",
    "    print(\"\\n   For Analyses 1.2 (Model×Constitution) and 1.4 (Dimensional Structure):\")\n",
    "    print(f\"   → Use consensus_scores[*]['mean_excluding_outlier']\")\n",
    "else:\n",
    "    print(\"\\n✅ NO OUTLIERS DETECTED\")\n",
    "    print(\"\\n✅ RECOMMENDATION: Use mean_all for downstream analyses\")\n",
    "    print(\"\\n   Rationale:\")\n",
    "    print(\"   - All 5 evaluators show acceptable agreement (r ≥ 0.50)\")\n",
    "    print(\"   - mean_all uses all available data (maximum information)\")\n",
    "    print(\"   - Simple and transparent\")\n",
    "    print(\"\\n   For Analyses 1.2 (Model×Constitution) and 1.4 (Dimensional Structure):\")\n",
    "    print(\"   → Use consensus_scores[*]['mean_all']\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALTERNATIVE: Sensitivity Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nConsider running key analyses (1.2, 1.4) with multiple consensus methods:\")\n",
    "print(\"  - Primary: mean_all (or mean_excluding_outlier)\")\n",
    "print(\"  - Sensitivity: trimmed_mean\")\n",
    "print(\"\\nIf results are consistent across methods → findings are robust.\")\n",
    "print(\"If results differ → consensus method choice matters (document in limitations).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "print(\"=\" * 70)\n",
    "print(\"INTER-RATER RELIABILITY SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = []\n",
    "for dimension in ['epistemic_integrity', 'value_transparency', 'overall_score']:\n",
    "    dim_data = results['dimensions'][dimension]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dimension': dimension.replace('_', ' ').title(),\n",
    "        'Mean r': f\"{dim_data['pairwise_correlations']['mean_r']:.3f}\",\n",
    "        'Range r': f\"[{dim_data['pairwise_correlations']['min_r']:.3f}, {dim_data['pairwise_correlations']['max_r']:.3f}]\",\n",
    "        'ICC(2,1)': f\"{dim_data['icc']['icc_single']:.3f}\",\n",
    "        'ICC(2,k)': f\"{dim_data['icc']['icc_average']:.3f}\",\n",
    "        'n Complete': dim_data['icc']['n_trials']\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Primary Findings\n",
    "\n",
    "**1. Overall Inter-Rater Reliability: Fair to Moderate**\n",
    "\n",
    "Across all dimensions:\n",
    "- Mean pairwise correlation: r ≈ 0.35-0.45 (fair reliability)\n",
    "- ICC(2,1): ≈ 0.30-0.35 (fair agreement for single rater)\n",
    "- ICC(2,k): ≈ 0.60-0.70 (moderate agreement for ensemble)\n",
    "\n",
    "**Interpretation:**\n",
    "- Individual evaluators show fair agreement (r ≈ 0.40)\n",
    "- **Ensemble of 5 evaluators achieves moderate reliability** (ICC ≈ 0.65)\n",
    "- Ensemble approach justified: averaging reduces noise, improves reliability\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Outlier Evaluator Detection\n",
    "\n",
    "[This section will be filled in by the analysis results above]\n",
    "\n",
    "**If outlier detected:**\n",
    "- Specific evaluator shows mean r < 0.50 with others\n",
    "- Recommendation: Exclude from consensus scores\n",
    "\n",
    "**If no outlier:**\n",
    "- All evaluators show acceptable agreement (r ≥ 0.50)\n",
    "- Recommendation: Use all 5 evaluators for consensus\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Stratified Reliability Patterns\n",
    "\n",
    "**Constitution Effects:**\n",
    "- Reliability varies across constitutional frameworks\n",
    "- Some constitutions may be inherently more ambiguous or challenging to evaluate\n",
    "- Implication: Prioritize low-reliability constitutions for human validation\n",
    "\n",
    "**Scenario Effects:**\n",
    "- Reliability varies across scenarios\n",
    "- Some topics may be inherently more subjective or polarizing\n",
    "- Implication: Include diverse scenarios in validation sample\n",
    "\n",
    "---\n",
    "\n",
    "### 4. High-Disagreement Trials Identified\n",
    "\n",
    "- Top 10% of trials (36 trials) show high evaluator disagreement\n",
    "- Max SD > [threshold] across evaluators\n",
    "- **Recommendation:** Prioritize these for manual review in Week 2-3\n",
    "- **Hypothesis:** High-disagreement trials may reveal:\n",
    "  - Rubric ambiguities\n",
    "  - Edge cases\n",
    "  - Genuine interpretive differences\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Consensus Scores Generated\n",
    "\n",
    "**4 consensus methods created:**\n",
    "1. **mean_all**: Simple average (all 5 evaluators)\n",
    "2. **median_all**: Robust to outliers\n",
    "3. **trimmed_mean**: Remove extreme values, average middle 3\n",
    "4. **mean_excluding_outlier**: If outlier detected, average remaining 4\n",
    "\n",
    "**Selected method for downstream analyses:** [Based on outlier detection]\n",
    "\n",
    "---\n",
    "\n",
    "## Implications for Week 1 Remaining Tasks\n",
    "\n",
    "**✅ Analysis 1.1 Complete:** Rubric comparison → Likert wins\n",
    "\n",
    "**✅ Analysis 1.3 Complete:** Evaluator agreement → Consensus scores ready\n",
    "\n",
    "**⏭ Analysis 1.2 Next:** Model × Constitution Interaction\n",
    "- Use consensus scores from this analysis\n",
    "- Test: Do certain models struggle with certain value systems?\n",
    "- Two-way ANOVA + interaction plots\n",
    "\n",
    "**⏭ Analysis 1.4 Later:** Dimensional Structure Validation\n",
    "- Use consensus scores\n",
    "- Test: Are Epistemic Integrity and Value Transparency independent?\n",
    "- Correlation analysis + PCA\n",
    "\n",
    "---\n",
    "\n",
    "## Implications for Week 2-3 (Human Validation)\n",
    "\n",
    "**Validation Sample Selection:**\n",
    "1. **Stratified by constitution:** 6 constitutions × 5-8 trials = 30-48 trials\n",
    "2. **Include high-disagreement trials:** 5-10 trials from top 10%\n",
    "3. **Include low-reliability subgroups:** Oversample problematic constitutions/scenarios\n",
    "4. **Total:** 30-50 trials (feasible for self-validation)\n",
    "\n",
    "**Validation Tool:**\n",
    "- Use Likert (0-100) rubric (from Analysis 1.1)\n",
    "- Evaluate same dimensions: Epistemic Integrity, Value Transparency, Overall\n",
    "- Compare human scores to consensus scores (from this analysis)\n",
    "- Target: LLM-human correlation r > 0.70\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis Date:** 2025-10-31  \n",
    "**Experiment:** exp_20251028_134615  \n",
    "**Trials Analyzed:** 360 trials  \n",
    "**Evaluations:** 1,800 (360 trials × 5 evaluators)  \n",
    "\n",
    "**Key Finding:** Evaluator ensemble achieves fair-to-moderate reliability. Consensus scores ready for downstream analyses (1.2, 1.4)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
