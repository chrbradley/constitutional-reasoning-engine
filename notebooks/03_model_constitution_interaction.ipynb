{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ã— Constitution Interaction Analysis\n",
    "\n",
    "**Week 1, Task 3 (Analysis 1.2) of Analysis & Publication Plan**\n",
    "\n",
    "**Research Question:** Do certain models perform differently with certain constitutional frameworks?\n",
    "\n",
    "**Dataset:** 360 trials with consensus scores (from Analysis 1.3)\n",
    "\n",
    "**Design:** 5 models Ã— 6 constitutions = 30 cells\n",
    "\n",
    "**Models Tested:**\n",
    "- Claude Sonnet 4.5\n",
    "- GPT-4o  \n",
    "- Gemini 2.5 Pro\n",
    "- Llama 3.1 405B\n",
    "- DeepSeek V3\n",
    "\n",
    "**Constitutions Tested:**\n",
    "- Balanced Justice\n",
    "- Community Order\n",
    "- Harm Minimization\n",
    "- No Constitution (control)\n",
    "- Self-Sovereignty\n",
    "- Utilitarian\n",
    "\n",
    "**Purpose:** Answer Q3 from research roadmap - Test if certain models excel with some value systems but struggle with others.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from analysis.interaction_analysis import InteractionAnalyzer\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Experiment ID\n",
    "EXPERIMENT_ID = 'exp_20251028_134615'\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Run Analysis\n",
    "\n",
    "This uses the `interaction_analysis.py` script to:\n",
    "- Load consensus scores from Analysis 1.3 (mean_all method)\n",
    "- Calculate mean scores for each Model Ã— Constitution cell\n",
    "- Run two-way ANOVA to test interaction effect\n",
    "- Perform post-hoc Tukey HSD tests\n",
    "- Analyze simple effects (per-model constitution differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete analysis\n",
    "analyzer = InteractionAnalyzer(EXPERIMENT_ID)\n",
    "results = analyzer.analyze(consensus_method=\"mean_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA Summary: Main Effects and Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ANOVA results for all dimensions\n",
    "anova_summary = []\n",
    "\n",
    "for dimension in ['epistemic_integrity', 'value_transparency', 'overall_score']:\n",
    "    anova = results['dimensions'][dimension]['anova']\n",
    "    \n",
    "    anova_summary.append({\n",
    "        'Dimension': dimension.replace('_', ' ').title(),\n",
    "        'Effect': 'Model',\n",
    "        'F': f\"{anova['anova_table']['model_effect']['F']:.2f}\",\n",
    "        'p': f\"{anova['anova_table']['model_effect']['p']:.6f}\",\n",
    "        'Î·Â²': f\"{anova['anova_table']['model_effect']['eta_sq']:.3f}\",\n",
    "        'Strength': anova['interpretation']['model_strength'],\n",
    "        'Significant': 'âœ…' if anova['interpretation']['model_effect_significant'] else 'âŒ'\n",
    "    })\n",
    "    \n",
    "    anova_summary.append({\n",
    "        'Dimension': dimension.replace('_', ' ').title(),\n",
    "        'Effect': 'Constitution',\n",
    "        'F': f\"{anova['anova_table']['constitution_effect']['F']:.2f}\",\n",
    "        'p': f\"{anova['anova_table']['constitution_effect']['p']:.6f}\",\n",
    "        'Î·Â²': f\"{anova['anova_table']['constitution_effect']['eta_sq']:.3f}\",\n",
    "        'Strength': anova['interpretation']['constitution_strength'],\n",
    "        'Significant': 'âœ…' if anova['interpretation']['constitution_effect_significant'] else 'âŒ'\n",
    "    })\n",
    "    \n",
    "    anova_summary.append({\n",
    "        'Dimension': dimension.replace('_', ' ').title(),\n",
    "        'Effect': 'Model Ã— Constitution',\n",
    "        'F': f\"{anova['anova_table']['interaction_effect']['F']:.2f}\",\n",
    "        'p': f\"{anova['anova_table']['interaction_effect']['p']:.6f}\",\n",
    "        'Î·Â²': f\"{anova['anova_table']['interaction_effect']['eta_sq']:.3f}\",\n",
    "        'Strength': anova['interpretation']['interaction_strength'],\n",
    "        'Significant': 'âœ…' if anova['interpretation']['interaction_significant'] else 'âŒ'\n",
    "    })\n",
    "\n",
    "df_anova = pd.DataFrame(anova_summary)\n",
    "df_anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Cell Means Heatmaps\n",
    "\n",
    "**Interpretation:**\n",
    "- Each cell shows mean score for Model Ã— Constitution combination\n",
    "- Darker green = higher scores (better performance)\n",
    "- Look for interaction patterns: Do some models excel/struggle with specific constitutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmaps for each dimension\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "dimensions = ['epistemic_integrity', 'value_transparency', 'overall_score']\n",
    "dimension_names = ['Epistemic Integrity', 'Value Transparency', 'Overall Score']\n",
    "\n",
    "for idx, (dimension, dimension_name) in enumerate(zip(dimensions, dimension_names)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get cell means\n",
    "    cell_means = pd.DataFrame(results['dimensions'][dimension]['cell_means'])\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        cell_means,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='RdYlGn',\n",
    "        vmin=60,\n",
    "        vmax=100,\n",
    "        center=80,\n",
    "        square=False,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'label': 'Mean Score'},\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(dimension_name, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Constitution', fontsize=11)\n",
    "    ax.set_ylabel('Model', fontsize=11)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Model Ã— Constitution Cell Means', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Look for vertical patterns (constitution effects) and horizontal patterns (model effects)\")\n",
    "print(\"ðŸ“Š Interaction: Does pattern change across rows/columns?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Interaction Plots\n",
    "\n",
    "**Classic interaction plot:**\n",
    "- X-axis: Models\n",
    "- Y-axis: Mean score\n",
    "- 6 lines: One per constitution\n",
    "\n",
    "**If lines are parallel:** No interaction (models perform consistently)\n",
    "**If lines cross/diverge:** Interaction present (different models excel with different constitutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe for plotting\n",
    "df = analyzer.build_dataframe(\"mean_all\")\n",
    "\n",
    "# Create interaction plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "constitutions = sorted(df['constitution'].unique())\n",
    "constitution_colors = sns.color_palette('husl', len(constitutions))\n",
    "constitution_color_map = dict(zip(constitutions, constitution_colors))\n",
    "\n",
    "for idx, (dimension, dimension_name) in enumerate(zip(dimensions, dimension_names)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Calculate mean scores per Model Ã— Constitution\n",
    "    interaction_data = df.groupby(['layer2_model', 'constitution'])[dimension].mean().reset_index()\n",
    "    \n",
    "    # Plot lines for each constitution\n",
    "    for constitution in constitutions:\n",
    "        const_data = interaction_data[interaction_data['constitution'] == constitution]\n",
    "        \n",
    "        ax.plot(\n",
    "            const_data['layer2_model'],\n",
    "            const_data[dimension],\n",
    "            marker='o',\n",
    "            markersize=8,\n",
    "            linewidth=2,\n",
    "            label=constitution,\n",
    "            color=constitution_color_map[constitution],\n",
    "            alpha=0.8\n",
    "        )\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_title(dimension_name, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Model', fontsize=11)\n",
    "    ax.set_ylabel('Mean Score', fontsize=11)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(60, 100)\n",
    "    \n",
    "    if idx == 2:  # Only show legend on last plot\n",
    "        ax.legend(title='Constitution', fontsize=9, loc='best', framealpha=0.9)\n",
    "\n",
    "plt.suptitle('Model Ã— Constitution Interaction Plots', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Parallel lines â†’ No interaction (consistent performance)\")\n",
    "print(\"ðŸ“Š Crossing/diverging lines â†’ Interaction present (different patterns per model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Effects Analysis: Per-Model Constitution Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract simple effects for overall_score\n",
    "simple_effects = results['dimensions']['overall_score']['simple_effects']\n",
    "\n",
    "simple_effects_summary = []\n",
    "for model, stats_dict in simple_effects.items():\n",
    "    simple_effects_summary.append({\n",
    "        'Model': model,\n",
    "        'Range (points)': f\"{stats_dict['range']:.2f}\",\n",
    "        'Best Constitution': stats_dict['best_constitution'],\n",
    "        'Worst Constitution': stats_dict['worst_constitution'],\n",
    "        'F-statistic': f\"{stats_dict['anova_F']:.2f}\",\n",
    "        'p-value': f\"{stats_dict['anova_p']:.4f}\",\n",
    "        'Constitutions Differ': 'âœ…' if stats_dict['constitutions_differ'] else 'âŒ'\n",
    "    })\n",
    "\n",
    "df_simple = pd.DataFrame(simple_effects_summary)\n",
    "df_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Per-Model Constitution Rankings\n",
    "\n",
    "Shows which constitutions each model performs best/worst with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped bar chart showing constitution performance per model\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "models = sorted(df['layer2_model'].unique())\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "        \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get mean scores per constitution for this model\n",
    "    model_data = df[df['layer2_model'] == model].groupby('constitution')['overall_score'].mean().sort_values()\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = ax.barh(\n",
    "        model_data.index,\n",
    "        model_data.values,\n",
    "        color=[constitution_color_map[const] for const in model_data.index],\n",
    "        edgecolor='black',\n",
    "        linewidth=1.5,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, model_data.values):\n",
    "        ax.text(\n",
    "            value + 0.5,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f'{value:.1f}',\n",
    "            ha='left',\n",
    "            va='center',\n",
    "            fontweight='bold',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_title(model, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Mean Overall Score', fontsize=10)\n",
    "    ax.set_xlim(60, 100)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Hide unused subplot\n",
    "if len(models) < len(axes):\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Constitution Performance by Model (Overall Score)', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Each subplot shows how one model ranks the constitutions\")\n",
    "print(\"ðŸ“Š If all models show similar rankings â†’ No interaction\")\n",
    "print(\"ðŸ“Š If rankings differ across models â†’ Interaction present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Hoc Tests: Significant Pairwise Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract post-hoc results for overall_score\n",
    "post_hoc = results['dimensions']['overall_score']['post_hoc']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POST-HOC TUKEY HSD TESTS (Overall Score)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nSIGNIFICANT MODEL DIFFERENCES: {len(post_hoc['models']['significant_pairs'])}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if post_hoc['models']['significant_pairs']:\n",
    "    model_pairs_df = pd.DataFrame(post_hoc['models']['significant_pairs'])\n",
    "    model_pairs_df = model_pairs_df.sort_values('mean_diff', key=abs, ascending=False)\n",
    "    display(model_pairs_df[['group1', 'group2', 'mean_diff', 'p_adj']].head(10))\n",
    "else:\n",
    "    print(\"No significant pairwise differences between models\")\n",
    "\n",
    "print(f\"\\n\\nSIGNIFICANT CONSTITUTION DIFFERENCES: {len(post_hoc['constitutions']['significant_pairs'])}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if post_hoc['constitutions']['significant_pairs']:\n",
    "    const_pairs_df = pd.DataFrame(post_hoc['constitutions']['significant_pairs'])\n",
    "    const_pairs_df = const_pairs_df.sort_values('mean_diff', key=abs, ascending=False)\n",
    "    display(const_pairs_df[['group1', 'group2', 'mean_diff', 'p_adj']].head(10))\n",
    "else:\n",
    "    print(\"No significant pairwise differences between constitutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key findings\n",
    "overall_anova = results['dimensions']['overall_score']['anova']\n",
    "overall_simple = results['dimensions']['overall_score']['simple_effects']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KEY FINDINGS: MODEL Ã— CONSTITUTION INTERACTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. MAIN EFFECT: MODEL\")\n",
    "print(\"-\"*70)\n",
    "if overall_anova['interpretation']['model_effect_significant']:\n",
    "    print(f\"âœ… SIGNIFICANT: Models differ in overall performance\")\n",
    "    print(f\"   F = {overall_anova['anova_table']['model_effect']['F']:.2f}, p = {overall_anova['anova_table']['model_effect']['p']:.6f}\")\n",
    "    print(f\"   Effect size: Î·Â² = {overall_anova['anova_table']['model_effect']['eta_sq']:.3f} ({overall_anova['interpretation']['model_strength']})\")\n",
    "else:\n",
    "    print(f\"âŒ NOT SIGNIFICANT: Models perform similarly overall\")\n",
    "    print(f\"   p = {overall_anova['anova_table']['model_effect']['p']:.6f}\")\n",
    "\n",
    "print(\"\\n2. MAIN EFFECT: CONSTITUTION\")\n",
    "print(\"-\"*70)\n",
    "if overall_anova['interpretation']['constitution_effect_significant']:\n",
    "    print(f\"âœ… SIGNIFICANT: Constitutions differ in resulting scores\")\n",
    "    print(f\"   F = {overall_anova['anova_table']['constitution_effect']['F']:.2f}, p = {overall_anova['anova_table']['constitution_effect']['p']:.6f}\")\n",
    "    print(f\"   Effect size: Î·Â² = {overall_anova['anova_table']['constitution_effect']['eta_sq']:.3f} ({overall_anova['interpretation']['constitution_strength']})\")\n",
    "else:\n",
    "    print(f\"âŒ NOT SIGNIFICANT: Constitutions produce similar scores\")\n",
    "    print(f\"   p = {overall_anova['anova_table']['constitution_effect']['p']:.6f}\")\n",
    "\n",
    "print(\"\\n3. INTERACTION EFFECT: MODEL Ã— CONSTITUTION\")\n",
    "print(\"-\"*70)\n",
    "if overall_anova['interpretation']['interaction_significant']:\n",
    "    print(f\"âœ… SIGNIFICANT INTERACTION DETECTED\")\n",
    "    print(f\"   F = {overall_anova['anova_table']['interaction_effect']['F']:.2f}, p = {overall_anova['anova_table']['interaction_effect']['p']:.6f}\")\n",
    "    print(f\"   Effect size: Î·Â² = {overall_anova['anova_table']['interaction_effect']['eta_sq']:.3f} ({overall_anova['interpretation']['interaction_strength']})\")\n",
    "    print(\"\\n   INTERPRETATION: Different models perform differently across constitutions!\")\n",
    "    print(\"   Some models excel with certain value systems but struggle with others.\")\n",
    "else:\n",
    "    print(f\"âŒ NO SIGNIFICANT INTERACTION\")\n",
    "    print(f\"   p = {overall_anova['anova_table']['interaction_effect']['p']:.6f}\")\n",
    "    print(\"\\n   INTERPRETATION: Models perform consistently across constitutions.\")\n",
    "    print(\"   Constitution effects are uniform - no model-specific patterns.\")\n",
    "\n",
    "print(\"\\n4. SIMPLE EFFECTS (PER-MODEL CONSTITUTION SENSITIVITY)\")\n",
    "print(\"-\"*70)\n",
    "for model, stats_dict in sorted(overall_simple.items(), key=lambda x: x[1]['range'], reverse=True):\n",
    "    significant = \"âœ…\" if stats_dict['constitutions_differ'] else \"âŒ\"\n",
    "    print(f\"{model:25} {significant} Range: {stats_dict['range']:5.2f} points \")\n",
    "    print(f\"{'':27} Best: {stats_dict['best_constitution']:20} Worst: {stats_dict['worst_constitution']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation and Implications\n",
    "\n",
    "### What Does This Mean?\n",
    "\n",
    "**If Interaction is Significant:**\n",
    "- Different models have different \"constitutional profiles\"\n",
    "- Model A might excel with Utilitarian values but struggle with Community Order\n",
    "- Model B might show the opposite pattern\n",
    "- **Implication:** Model choice matters depending on which value system you want to reason from\n",
    "- **For AI Alignment:** Models are not interchangeable - they have value-specific strengths/weaknesses\n",
    "\n",
    "**If Interaction is NOT Significant:**\n",
    "- Models perform consistently across constitutions\n",
    "- Constitution effects are uniform (same pattern for all models)\n",
    "- **Implication:** Model choice less critical - effects are additive (model quality + constitution difficulty)\n",
    "- **For AI Alignment:** Models are roughly interchangeable for constitutional reasoning tasks\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison to Q1 (Model Performance) and Q2 (Constitution Effects)\n",
    "\n",
    "**Q1: Model Performance (Main Effect)**\n",
    "- Tests: Do some models score higher overall?\n",
    "- What it tells us: Which models are \"better\" at constitutional reasoning in general\n",
    "\n",
    "**Q2: Constitution Effects (Main Effect)**\n",
    "- Tests: Do some constitutions lead to higher/lower scores?\n",
    "- What it tells us: Which value systems are \"easier\" or \"harder\" for models\n",
    "\n",
    "**Q3: Model Ã— Constitution Interaction (This Analysis)**\n",
    "- Tests: Do model rankings change across constitutions?\n",
    "- What it tells us: Whether models have constitution-specific strengths/weaknesses\n",
    "- **More interesting than Q1/Q2:** Reveals nuanced patterns beyond \"this model is better\"\n",
    "\n",
    "---\n",
    "\n",
    "### Implications for Human Validation (Week 2-3)\n",
    "\n",
    "**If Interaction Found:**\n",
    "- Validation sample should include diverse Model Ã— Constitution combinations\n",
    "- Prioritize cells with extreme scores (very high or very low)\n",
    "- Test hypothesis: Do humans also perceive constitution-specific model performance?\n",
    "\n",
    "**If No Interaction:**\n",
    "- Validation can focus on main effects (best/worst models, easiest/hardest constitutions)\n",
    "- Simpler sampling strategy: stratify by model OR constitution, not both\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**âœ… Analysis 1.1 Complete:** Rubric Comparison â†’ Likert wins\n",
    "\n",
    "**âœ… Analysis 1.3 Complete:** Evaluator Agreement â†’ Consensus scores ready\n",
    "\n",
    "**âœ… Analysis 1.2 Complete:** Model Ã— Constitution Interaction â†’ [Results from this notebook]\n",
    "\n",
    "**â­ Analysis 1.4 Next:** Dimensional Structure Validation\n",
    "- Test: Are Epistemic Integrity and Value Transparency independent?\n",
    "- Correlation analysis + PCA\n",
    "- Validates 2-dimensional rubric design\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis Date:** 2025-10-31  \n",
    "**Experiment:** exp_20251028_134615  \n",
    "**Trials Analyzed:** 360 trials with consensus scores  \n",
    "**Design:** 5 models Ã— 6 constitutions = 30 cells\n",
    "\n",
    "**Key Question Answered:** Do certain models perform differently with certain constitutional frameworks?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Constitution Project",
   "language": "python",
   "name": "constitution"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
