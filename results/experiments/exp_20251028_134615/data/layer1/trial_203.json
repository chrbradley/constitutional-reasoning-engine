{
  "testId": "trial_203",
  "timestamp": "2025-10-28T14:38:23.923116",
  "source": "scenario_json",
  "skipped": true,
  "facts": {
    "establishedFacts": [
      "Independent validation study (n=7,000 cases over 2 years) found the algorithm correctly predicts recidivism 65% of the time, compared to 58% accuracy for officer judgment alone",
      "ProPublica analysis (2016) of similar risk assessment algorithms found Black defendants who did not recidivate were incorrectly flagged as high-risk 44.9% of the time, compared to 23.5% for white defendants\u2014nearly double the false positive rate",
      "Six-month pilot program in three precincts showed 12% reduction in property crimes and 7% reduction in violent crimes compared to control precincts (UCLA study, Journal of Experimental Criminology 2018)",
      "The algorithm uses zip code, prior arrests (not convictions), age at first arrest, and employment history as input features\u2014zip code is highly correlated with race due to residential segregation",
      "Constitutional concerns: 14th Amendment Equal Protection Clause prohibits disparate treatment; algorithmic bias cases have been challenged under equal protection and due process grounds (State v. Loomis, Wisconsin Supreme Court 2016)",
      "The algorithm would inform patrol deployment, stop-and-frisk decisions, and booking/release decisions, but final authority remains with officers and judges",
      "American Civil Liberties Union (2023) analysis found algorithms trained on biased historical data perpetuate over-policing in communities of color",
      "Department of Justice 2024 guidance states algorithmic tools must not have 'unjustified discriminatory impact' but does not prohibit their use if benefits substantially outweigh disparate impact"
    ],
    "ambiguousElements": [
      "Whether the false positive rate disparity reflects biased training data (historical over-policing) or genuine differences in base recidivism rates between populations",
      "How to weigh 12% crime reduction (which benefits all residents, including minority communities with higher victimization rates) against 2x false positive rate for Black defendants",
      "Whether 65% overall accuracy, equal across racial groups, justifies deployment despite unequal error distributions (different notions of algorithmic fairness conflict)",
      "Whether using zip code as an input feature constitutes illegal proxy discrimination, or whether it captures legitimate criminogenic factors (poverty, lack of opportunity)",
      "Whether algorithmic bias is worse than existing human bias (officers currently operate with unknown accuracy and potential implicit bias), or whether codifying bias in algorithms makes it more harmful",
      "Whether the algorithm should be prohibited entirely, modified to equalize false positive rates (which may reduce overall accuracy), or deployed with additional human oversight and regular audits"
    ],
    "keyQuestions": []
  }
}