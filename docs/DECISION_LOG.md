# Research Decision Log

**Purpose:** Track all major decisions made during the research project with rationale and evidence

**Format:** Most recent decisions at top

---

## Decision Template

```markdown
### Decision #X: [Title]
**Date:** YYYY-MM-DD
**Phase:** [Phase number]
**Context:** [Why did this decision need to be made?]
**Options Considered:**
1. Option A: [Description]
   - Pros:
   - Cons:
2. Option B: [Description]
   - Pros:
   - Cons

**Evidence:**
- [Data, analysis, or literature supporting decision]

**Decision:** [What was chosen]

**Rationale:** [Why this option was selected]

**Impact:** [Expected consequences of this decision]

**Reversible:** YES / NO
**If reversible:** [Conditions under which we'd reverse]
```

---

## Decisions

### Decision #1: Research Direction Pivot
**Date:** 2025-10-26
**Phase:** Pre-Phase 0
**Context:**
After analyzing Phase 1 evaluator comparison results, discovered mean inter-evaluator correlation r=0.632 (moderate-good agreement) with 119 trials, much better than initial 24-trial analysis showing r=0.061. This raised question: Continue with constitutional adherence or pivot to LLM-as-judge validation research?

**Options Considered:**
1. **Focus on constitutional adherence** (original goal)
   - Pros: Original research question, data collection nearly complete, r=0.632 is adequate
   - Cons: Validity not yet established (no human ground truth)

2. **Pivot entirely to LLM-as-judge validation**
   - Pros: Broader applicability, publishable methodology, addresses research gaps
   - Cons: Abandons constitutional adherence work, requires human annotation

3. **Dual-track approach** (constitutional adherence first, then methodology)
   - Pros: Completes original research, then extends to methodology
   - Cons: Longer timeline, more complex

**Evidence:**
- Literature: Typical LLM-LLM correlations r=0.27-0.46 (our r=0.632 is above norm)
- Sample size: n=119 adequate for correlation (CI ≈ ±0.10), but n≥200 ideal
- Evaluator convergence: 4 of 5 evaluators show r=0.72-0.80 agreement (Gemini Pro outlier)
- 7 research gaps identified in LLM-as-judge literature

**Decision:** Dual-track approach (sequential, not parallel)

**Rationale:**
1. Constitutional adherence is feasible NOW (validated evaluators via convergent agreement)
2. r=0.632 is adequate for exploratory research (above literature norms)
3. LLM-as-judge research best informed BY constitutional adherence learnings
4. Demonstrates both applied and foundational research skills

**Impact:**
- Immediate: Focus on completing constitutional adherence (Phases 0-5)
- Future: Optional methodological research (Phase 6) after publication
- Career: Positions user for both research and engineering roles at Anthropic

**Reversible:** Partially
**If reversible:** If Phase 4 validity check fails (LLM-human r < 0.50), must pivot to methodology or redesign

---

### Decision #2: Sample Size Interpretation Correction
**Date:** 2025-10-26
**Phase:** Pre-Phase 0 (Analysis of existing data)

**Context:**
Initial evaluator comparison was run on n=24 trials (single constitution: harm-minimization only), showing catastrophically low inter-evaluator correlation (mean r=0.061). This led to conclusion that evaluators disagreed fundamentally. However, full dataset analysis (n=119 across all constitutions) showed mean r=0.632, suggesting initial sample was statistically underpowered.

**Options Considered:**
1. **Trust small sample** (n=24)
   - Pros: First analysis, more recent
   - Cons: Violates minimum sample size requirements (n≥100 for stable correlation)

2. **Trust large sample** (n=119)
   - Pros: Adequate sample size, more representative
   - Cons: Questions why small sample differed so dramatically

3. **Investigate discrepancy**
   - Pros: Understand root cause (constitution-specific effect vs sampling error)
   - Cons: Time-intensive

**Evidence:**
- Statistical principle: Correlation estimates require n≥100 for CI ≈ ±0.10
- With n=24, CI ≈ ±0.35 (huge uncertainty - measured r=0.06 could actually be r=0.41)
- Literature: "Minimum sample sizes for reliable correlation estimates"
- Small sample was harm-minimization only (not representative of all constitutions)

**Decision:** Trust large sample (n=119, r=0.632), treat small sample as statistical artifact

**Rationale:**
- Sample size principle: n=24 is too small for stable correlation estimate
- Sampling error: Random fluctuation around true correlation likely caused low r
- Constitution-specific: Harm-min might genuinely differ (requires stratified analysis in Phase 1)

**Impact:**
- Positive: Research is NOT fundamentally flawed (evaluators DO agree)
- Methodological: Establishes need for Phase 1 diagnostic (stratified analysis)
- Learning: Added sample size requirements to methodological guidelines (Phase 0.1)

**Reversible:** NO (statistical fact)
**Lesson:** ALWAYS check sample size adequacy before drawing conclusions. Added to CLAUDE.md guidelines.

---

### Decision #3: Phased Sequential Approach
**Date:** 2025-10-26
**Phase:** Pre-Phase 0 (Planning)

**Context:**
User identified massive decision tree of potential experimental directions:
- Evaluation design (binary vs Likert rubrics)
- Sample size expansion (add 6 scenarios)
- Validity establishment (human ground truth)
- Methodological research (5 proposed experiments)

Multiple experiments could be run, but doing everything at once would be chaotic and violate "one variable at a time" principle.

**Options Considered:**
1. **Run all experiments in parallel**
   - Pros: Fastest completion
   - Cons: Violates scientific principles, can't isolate effects, overwhelming

2. **Pick one experiment and ignore others**
   - Pros: Focused
   - Cons: Misses opportunities, decision tree not explored

3. **Sequential phased approach with decision points**
   - Pros: Systematic, scientifically sound, adapts based on findings
   - Cons: Longer timeline

**Evidence:**
- Scientific method: One variable at a time
- Risk management: Fail fast at each phase (don't invest in bad design)
- Resource efficiency: Each phase informs next (avoid wasted work)

**Decision:** Sequential phased approach (Phases 0-6 with decision points)

**Rationale:**
- **Phase 0 (Foundation):** Establish methodological rigor FIRST (prevents future errors)
- **Phase 1 (Diagnosis):** Understand current data BEFORE changing anything
- **Phase 2 (Design Test):** Empirically test eval redesign BEFORE scaling up
- **Phase 3 (Sample Expansion):** Add data with finalized design
- **Phase 4 (Validity):** Establish ground truth (can't skip this)
- **Phase 5 (Publication):** Answer original question with validated methodology
- **Phase 6 (Optional):** Methodological contributions after main research complete

**Impact:**
- Timeline: 10-12 weeks to publication (systematic but not rushed)
- Quality: Each phase validated before proceeding (fail-safe mechanism)
- Flexibility: Decision points allow pivots based on findings
- Documentation: Clear roadmap for user and AI to follow

**Reversible:** Partially
**If needed:** Can skip Phase 6 (optional), compress Phases 2-3 if eval test fails

---

### Decision #4: Use Standard Statistical Libraries Instead of Custom Tools
**Date:** 2025-10-27
**Phase:** Phase 0.1 (Methodological Guidelines)
**Context:**
Phase 0.1 tasks 0.1.2 and 0.1.3 originally proposed building custom sample size calculators (`sample_size_calculator.py`) and confidence interval calculators (`ci_calculator.py`). User questioned whether these tools needed to be built or if existing libraries already provided these capabilities.

**Options Considered:**
1. **Build custom tools** (as originally planned in RESEARCH_ROADMAP.md)
   - Pros: Tailored to our specific needs, self-contained
   - Cons: Reinventing the wheel, maintenance burden, potential for bugs, time-intensive

2. **Use canonical Python statistical libraries** (scipy, statsmodels, pingouin)
   - Pros: Industry standard, 20+ years of testing, universally used in research, no implementation time
   - Cons: External dependencies (minimal concern)

**Evidence:**
- **SciPy (`scipy.stats`)**: Core scientific Python stack, contains all fundamental statistical tests (correlations, CI calculations via pearsonr)
- **Statsmodels (`statsmodels.stats.power`)**: THE standard for statistical modeling in Python, includes power analysis and sample size calculations
- **Pingouin (`pingouin`)**: Built on scipy/statsmodels, provides simpler API for common tasks (correlation with CI in one call)
- These libraries are used universally in published research, academia, and industry
- No need to implement Fisher's z-transformation, bootstrap CI, power analysis formulas manually

**Decision:** Use standard statistical libraries (scipy, statsmodels, pingouin) instead of building custom tools

**Rationale:**
1. These are canonical, battle-tested libraries used across all scientific Python work
2. No value in reimplementing well-established statistical formulas
3. Faster development - skip implementation, add dependencies, use directly
4. Better reliability - 20+ years of testing vs custom implementation
5. Better documentation - extensive examples and academic references
6. Phase 0.1 goal is methodological rigor, not tool development

**Impact:**
- **Immediate:** Skip tasks 0.1.2 and 0.1.3 (no custom tool development)
- **Dependencies:** Add scipy, statsmodels, pingouin to pyproject.toml
- **Usage:** Import directly in analysis scripts when needed (e.g., `from scipy.stats import pearsonr`)
- **Timeline:** Saves ~4-6 hours of implementation time
- **Phase 0.1 completion:** Can mark as complete with methodological guidelines in CLAUDE.md

**Reversible:** YES (but highly unlikely)
**If reversed:** Could build custom wrappers later if specific needs arise, but standard libraries cover all current requirements

---

### Decision #5: Enhanced Data Schema - Self-Contained Files and Accessible Explanations
**Date:** 2025-10-27
**Phase:** Phase 0.2 (Data Architecture Redesign)

**Context:**
After successful migration to sequential trial IDs, user feedback identified two UX issues:
1. Layer files required trial_registry.json lookup to see scenario/model/constitution context
2. Layer 3 explanations/examples buried in markdown-wrapped JSON (hard to access for qualitative analysis)

This created friction for human inspection and would complicate future qualitative research phases.

**Options Considered:**

**Metadata Propagation:**
1. **Keep metadata in registry only** (original design)
   - Pros: No data duplication, single source of truth
   - Cons: Requires lookup for every file inspection, complicates analysis code

2. **Propagate to all layers** (chosen)
   - Pros: Self-contained files, easier human inspection, simpler analysis code
   - Cons: Minimal data duplication (3 short strings per file, ~30 bytes)

**Layer 3 Structure:**
1. **Keep only scores in response_parsed** (original)
   - Pros: Minimal, clean for statistical analysis
   - Cons: Explanations buried in raw markdown, hard qualitative analysis

2. **Break out explanations + examples** (chosen)
   - Pros: Enables qualitative analysis, human validation, debugging evaluator disagreements
   - Cons: Larger files (~15-20% increase, negligible)

**Evidence:**
- **Research workflow:** Phase 4 human validation will need to see LLM explanations alongside scores
- **Debugging needs:** Understanding evaluator disagreements requires reading reasoning, not just comparing numbers
- **Publication requirements:** Will need to cherry-pick insightful explanations for discussion sections
- **File size impact:** Testing showed ~2-3KB increase per Layer 3 trial (119 trials × 3KB = 357KB total, negligible)
- **Analysis simplification:** No registry lookups means analysis scripts can process files independently

**Decision:**
1. Propagate trial metadata (scenario_id, model, constitution) to all layer files
2. Expand Layer 3 response_parsed to include full structure: `{"factual_adherence": {"score": 72, "explanation": "...", "examples": [...]}, ...}`

**Rationale:**
1. **Self-contained files reduce cognitive load** - Researchers can inspect any file without cross-referencing registry
2. **Analysis code simplification** - No need for registry lookups in every analysis function
3. **Future-proofing** - Explanations will definitely be needed for Phase 4 validation and paper writing
4. **Storage cost negligible** - ~400KB total increase for 119 trials (0.01% of typical dataset)
5. **Matches original backup format** - Layer 3 backup files already had this structure, migration just extracts it properly

**Impact:**
- **Schema changes:** Layer2Data +1 field (scenario_id), Layer3Data +3 fields (scenario_id, model, constitution)
- **Migration:** Re-ran with enhanced schema (119 trials, all verified with manual inspection)
- **File size:** +15-20% per Layer 3 file (acceptable, still < 10KB per trial)
- **Analysis readiness:** Can now do both quantitative (scores) AND qualitative (explanations) analysis without switching between files
- **Human validation ready:** Phase 4 raters can see complete context + LLM reasoning in single file
- **Code updates needed:** Future runner.py updates (Phase 3) will need to include these fields

**Reversible:** YES (but unlikely)
**If reversed:** Could move back to registry-only design and scores-only structure, but would lose all UX benefits. More likely: keep this design and expand it (e.g., add Layer 1 explanations in future phases).

---

### Decision #6: Defer Phase 0.3 and Visualization Module
**Date:** 2025-10-27
**Phase:** Phase 0.4 (Diagnostic Analysis Tools)

**Context:**
Phase 0 roadmap originally included:
- 0.3: Build evaluation strategy plugin system (evaluation_strategies.py, experiment_config.py)
- 0.4: Build ALL analysis modules including publication-quality visualization suite

User questioned whether visualization module needed to be built now, since we're not yet presenting results publicly - still gathering initial insights to guide future experiments.

**Options Considered:**
1. **Complete Phase 0 as written** (0.3 + full 0.4 with visualization)
   - Pros: Full foundation infrastructure complete
   - Cons: ~3-4 days work before getting to actual data analysis, premature polish

2. **Defer 0.3 to Phase 3, build core 0.4 modules only** (no visualization)
   - Pros: Saves ~3-4 days, gets to Phase 1 diagnostics faster, builds only what's needed
   - Cons: Will need to build deferred modules before Phase 2-3

3. **Minimal 0.4 with ad-hoc analysis**
   - Pros: Fastest to insights
   - Cons: May need to rebuild tools mid-Phase 1, messy ad-hoc code

**Evidence:**
- Phase 1 genuinely needs 3 modules: stratified_analysis.py, outlier_detection.py, dimensionality.py
- Phase 1 does NOT need: evaluation strategies (used in Phase 2), publication viz (used in Phase 5)
- Previous Decision #5 successfully used "defer until needed" approach for runner/state updates
- Quick matplotlib/seaborn plots sufficient for Phase 1 diagnostic work
- Publication-quality visualizations only needed for Phase 5 paper/blog writing

**Decision:** Option 2 - Defer 0.3 to Phase 3, build 3 core analysis modules (no formal visualization module)

**Rationale:**
1. Pragmatic approach: Build tools when needed, not speculatively
2. Phase 1 only needs diagnostic analysis (correlations, outliers, PCA), not experiment running or publication figures
3. Evaluation strategies not needed until Phase 2 pilot testing (multi-prompt vs single-prompt comparison)
4. Visualization module not needed until Phase 5 publication
5. Faster path to insights: Can start Phase 1 immediately after building 3 modules (~1.5 days vs 4-5 days)
6. Follows same successful pattern as Decision #5 (defer runner updates to Phase 3)

**Impact:**
**Immediate:**
- Phase 0.4 scope: 3 modules (stratified_analysis, outlier_detection, dimensionality) instead of 5
- Timeline: Saves ~3-4 days, Phase 0 → Phase 1 transition accelerated
- Code quality: Still building proper modules (not ad-hoc), just deferring non-essential ones

**Deferred to Later:**
- evaluation_strategies.py → Phase 3 (when creating new experiments)
- experiment_config.py → Phase 3 (when creating new experiments)
- visualize_stratified.py → Phase 5 (when writing paper/blog)

**Phase 0 Completion:**
- Considered complete with 3 analysis modules built and tested
- Phase 1 diagnostic work can proceed immediately

**Reversible:** YES
**If needed:** Can build deferred modules anytime. Evaluation strategies needed before Phase 2 pilot, visualization before Phase 5 publication. More likely to expand than reverse (e.g., add more analysis utilities as Phase 1 progresses).

---

## Decision Index

Quick reference to find decisions:

| # | Title | Date | Phase | Reversible |
|---|-------|------|-------|------------|
| 1 | Research Direction Pivot | 2025-10-26 | Pre-0 | Partially |
| 2 | Sample Size Interpretation | 2025-10-26 | Pre-0 | NO |
| 3 | Phased Sequential Approach | 2025-10-26 | Pre-0 | Partially |
| 4 | Use Standard Statistical Libraries | 2025-10-27 | 0.1 | YES |
| 5 | Enhanced Data Schema | 2025-10-27 | 0.2 | YES |
| 6 | Defer Phase 0.3 and Visualization | 2025-10-27 | 0.4 | YES |

---

## How to Use This Log

**When making a decision:**
1. Create new entry at top (above this section)
2. Use template format
3. Number sequentially
4. Add to decision index
5. Reference in RESEARCH_ROADMAP.md

**When reviewing past decisions:**
1. Check decision index for quick reference
2. Read full entry for context and rationale
3. Note if decision was reversed (add update to original entry)

**When reversing a decision:**
1. Add "UPDATE" section to original decision entry
2. Explain why reversed and what new evidence emerged
3. Create new decision entry for the reversal

---

**Last Updated:** 2025-10-27
**Total Decisions:** 6
**Active Decisions:** 6
**Reversed Decisions:** 0
